{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_day1_OR gate",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsH4cfnD2_oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OR gate neural network implementation\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#input values\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]]) #(4,2) matrix\n",
        "Y = np.array([[0],[1],[1],[1]])  #(4,1) matrix (rowsXcolumns)\n",
        "nx = X.shape[0]\n",
        "ny = Y.shape[0]\n",
        "nh =  4\n",
        "#initialiing parameters\n",
        "def initialize_params(nx,nh,ny):\n",
        "  np.random.seed(3)\n",
        " \n",
        "  parameters={}\n",
        "  W1 = np.random.randn(nh,nx)*0.01\n",
        "  b1 = np.zeros(shape=(nh,1))\n",
        "  W2 = np.random.randn(ny,nh)*0.01\n",
        "  b2 = np.zeros(shape=(ny,1))\n",
        "  parameters = {'W1':W1,'b1':b1,'W2':W2,'b2':b2}\n",
        "  return parameters\n",
        "\n",
        "#activation function\n",
        "def sigmoid(z,deriv=False):\n",
        "  if (deriv==True):\n",
        "    return z*(1-z)\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "#feed forward\n",
        "def feed_forward(X,parameters):\n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "  cache = {}\n",
        "  Z1 = np.dot(W1,X)+b1  \n",
        "  A1 = sigmoid(Z1)\n",
        "  Z2 = np.dot(W2,A1)+b2\n",
        "  A2 = sigmoid(Z2)\n",
        "  cache={'Z1':Z1,'A1':A1,'Z2':Z2,'A2':A2}\n",
        "  return A2,cache\n",
        "\n",
        "#evaluating cost function\n",
        "def costFunction(Y,A2,parameters):\n",
        "  W1 = parameters['W1']\n",
        "  W2 = parameters['W2']\n",
        "  m = len(Y)  #no of examples\n",
        "  cost = -1/m *np.sum(Y*np.log(A2)+(1-Y)*np.log(1-A2))\n",
        "  cost = np.squeeze(cost)\n",
        "  return cost\n",
        "\n",
        "#backward propagation\n",
        "def feed_backward(parameters,cache,X,Y):\n",
        "  grads={}\n",
        "  A1 = cache['A1']\n",
        "  A2 = cache['A2']\n",
        "  gZ1 = sigmoid(A1,deriv=True)\n",
        "  #derivatives\n",
        "  dZ2 = A2 -Y\n",
        "  dW2 = 1/m*(np.dot(dZ2,A1.T))\n",
        "  db2 = 1/m*(np.sum(dZ2,axis=1,keepdims=True))\n",
        "  dZ1 = np.dot(W2.T,dZ2)*gZ1\n",
        "  dW1 = 1/m*(np.dot(dZ1,X.T))\n",
        "  db1 = 1/m*(np.sum(dZ1,axis=1,keepdims=True))\n",
        "  grads={'dW1':dW1,'db1':db1,'db2':db2,'dW2':dW2}\n",
        "  return grads\n",
        "\n",
        "#gradient descent and updating parameters\n",
        "def update(parameters,grads):\n",
        "  alpha = 0.1\n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "  dW1 = grads['dW1']\n",
        "  dW2 = grads['dW2']\n",
        "  db1 =grads['db1']\n",
        "  db2 =grads['db2']\n",
        "  #descent step\n",
        "  W1 = W1 - alpha * dW1\n",
        "  W2 = W2 - alpha * dW2\n",
        "  b1 = b1 - alpha * db1\n",
        "  b2 = b2 = alpha * db2\n",
        "  parameters = {'W1':W1,'W2':W2,'b1':b1,'b2':b2}  #update the parameters\n",
        "  return parameters\n",
        "\n",
        "#main neural network build\n",
        "def neural_net(X,Y,nh,iter=10000,print_cost=False): #main\n",
        "  parameters = initialize_params(nx,nh,ny)\n",
        "  W1 = parameters['W1']\n",
        "  b1 = parameters['b1']\n",
        "  W2 = parameters['W2']\n",
        "  b2 = parameters['b2']\n",
        "  for i in range(0,iter):\n",
        "    A2,cache = feed_forward(X,parameters) #feedforward\n",
        "    cost = costFunction(Y,A2,parameters)  #cost function\n",
        "    grads = feed_backward(parameters,cache,X,Y) #backpropagation\n",
        "    parameters = update(parameters,grads) #gradient descent\n",
        "    \n",
        "    if print_cost and i%1000==0:\n",
        "      print(\"Cost after iteration %i: %f\" % (i, cost))    \n",
        "  return parameters\n",
        "  \n",
        "neural_net(X,Y,nh,iter=10000,print_cost=True)  \n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "\n",
        "#prediction\n",
        "#def predict(parameters,X):\n",
        "  #A2,cache = feed_forward(X,parameters)\n",
        "  #predictions = np.round(A2)\n",
        "  #return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCpoG9uI4QCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}