{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_with_OpenAI_gym",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBdh2mgzWZbu"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from collections import deque\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-8uwKx9Wd_f",
        "outputId": "9d0566f6-b578-4037-8d81-f180c142e6ce"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "print(\"Action Space: {}\".format(env.action_space))\n",
        "print(\"State space: {}\".format(env.observation_space))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(2)\n",
            "State space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1yMJUqgW1VP"
      },
      "source": [
        "# An episode a full game\n",
        "train_episodes = 300\n",
        "test_episodes = 100\n",
        "\n",
        "def agent(state_shape, action_shape):\n",
        "    \"\"\" The agent maps X-states to Y-actions\n",
        "    e.g. The neural network output is [.1, .7, .1, .3]\n",
        "    The highest value 0.7 is the Q-Value.\n",
        "    The index of the highest action (0.7) is action #1.\n",
        "    \"\"\"\n",
        "    learning_rate = 0.001\n",
        "    init = tf.keras.initializers.HeUniform()\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_qs(model, state, step):\n",
        "    return model.predict(state.reshape([1, state.shape[0]]))[0]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27HKwSrgW80u"
      },
      "source": [
        "def train(env, replay_memory, model, target_model, done):\n",
        "    learning_rate = 0.7 # Learning rate\n",
        "    discount_factor = 0.618\n",
        "\n",
        "    MIN_REPLAY_SIZE = 1000\n",
        "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
        "        return\n",
        "\n",
        "    batch_size = 64 * 2\n",
        "    mini_batch = random.sample(replay_memory, batch_size)\n",
        "    current_states = np.array([encode_observation(transition[0], env.observation_space.shape) for transition in mini_batch])\n",
        "    current_qs_list = model.predict(current_states)\n",
        "    new_current_states = np.array([encode_observation(transition[3], env.observation_space.shape) for transition in mini_batch])\n",
        "    future_qs_list = target_model.predict(new_current_states)\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
        "        if not done:\n",
        "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
        "        else:\n",
        "            max_future_q = reward\n",
        "\n",
        "        current_qs = current_qs_list[index]\n",
        "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
        "\n",
        "        X.append(encode_observation(observation, env.observation_space.shape))\n",
        "        Y.append(current_qs)\n",
        "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)\n",
        "\n",
        "def encode_observation(observation, n_dims):\n",
        "    return observation"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P2-t-l-XB9u",
        "outputId": "d28651cc-d013-4058-f480-3658e17fd999"
      },
      "source": [
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "def main():\n",
        "    epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
        "    max_epsilon = 1 # You can't explore more than 100% of the time\n",
        "    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n",
        "    decay = 0.01\n",
        "\n",
        "    # 1. Initialize the Target and Main models\n",
        "    # Main Model (updated every step)\n",
        "    model = agent(env.observation_space.shape, env.action_space.n)\n",
        "    # Target Model (updated every 100 steps)\n",
        "    target_model = agent(env.observation_space.shape, env.action_space.n)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "\n",
        "    replay_memory = deque(maxlen=50_000)\n",
        "\n",
        "    target_update_counter = 0\n",
        "\n",
        "    # X = states, y = actions\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    steps_to_update_target_model = 0\n",
        "\n",
        "    for episode in range(train_episodes):\n",
        "        total_training_rewards = 0\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            steps_to_update_target_model += 1\n",
        "            #if True:\n",
        "            #env.render()\n",
        "\n",
        "            random_number = np.random.rand()\n",
        "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
        "            if random_number <= epsilon:\n",
        "                # Explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # Exploit best known action\n",
        "                # model dims are (batch, env.observation_space.n)\n",
        "                encoded = encode_observation(observation, env.observation_space.shape[0])\n",
        "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
        "                predicted = model.predict(encoded_reshaped).flatten()\n",
        "                action = np.argmax(predicted)\n",
        "            new_observation, reward, done, info = env.step(action)\n",
        "            replay_memory.append([observation, action, reward, new_observation, done])\n",
        "\n",
        "            # 3. Update the Main Network using the Bellman Equation\n",
        "            if steps_to_update_target_model % 4 == 0 or done:\n",
        "                train(env, replay_memory, model, target_model, done)\n",
        "\n",
        "            observation = new_observation\n",
        "            total_training_rewards += reward\n",
        "\n",
        "            if done:\n",
        "                print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))\n",
        "                total_training_rewards += 1\n",
        "\n",
        "                if steps_to_update_target_model >= 100:\n",
        "                    print('Copying main network weights to the target network weights')\n",
        "                    target_model.set_weights(model.get_weights())\n",
        "                    steps_to_update_target_model = 0\n",
        "                break\n",
        "\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training rewards: 14.0 after n steps = 0 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 1 with final reward = 1.0\n",
            "Total training rewards: 23.0 after n steps = 2 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 3 with final reward = 1.0\n",
            "Total training rewards: 31.0 after n steps = 4 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 5 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 31.0 after n steps = 6 with final reward = 1.0\n",
            "Total training rewards: 33.0 after n steps = 7 with final reward = 1.0\n",
            "Total training rewards: 38.0 after n steps = 8 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 14.0 after n steps = 9 with final reward = 1.0\n",
            "Total training rewards: 35.0 after n steps = 10 with final reward = 1.0\n",
            "Total training rewards: 17.0 after n steps = 11 with final reward = 1.0\n",
            "Total training rewards: 15.0 after n steps = 12 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 13 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 14 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 14.0 after n steps = 15 with final reward = 1.0\n",
            "Total training rewards: 15.0 after n steps = 16 with final reward = 1.0\n",
            "Total training rewards: 26.0 after n steps = 17 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 18 with final reward = 1.0\n",
            "Total training rewards: 28.0 after n steps = 19 with final reward = 1.0\n",
            "Total training rewards: 20.0 after n steps = 20 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 28.0 after n steps = 21 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 22 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 23 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 24 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 25 with final reward = 1.0\n",
            "Total training rewards: 18.0 after n steps = 26 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 10.0 after n steps = 27 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 28 with final reward = 1.0\n",
            "Total training rewards: 8.0 after n steps = 29 with final reward = 1.0\n",
            "Total training rewards: 15.0 after n steps = 30 with final reward = 1.0\n",
            "Total training rewards: 23.0 after n steps = 31 with final reward = 1.0\n",
            "Total training rewards: 19.0 after n steps = 32 with final reward = 1.0\n",
            "Total training rewards: 19.0 after n steps = 33 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 12.0 after n steps = 34 with final reward = 1.0\n",
            "Total training rewards: 17.0 after n steps = 35 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 36 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 37 with final reward = 1.0\n",
            "Total training rewards: 24.0 after n steps = 38 with final reward = 1.0\n",
            "Total training rewards: 48.0 after n steps = 39 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 15.0 after n steps = 40 with final reward = 1.0\n",
            "Total training rewards: 15.0 after n steps = 41 with final reward = 1.0\n",
            "Total training rewards: 16.0 after n steps = 42 with final reward = 1.0\n",
            "Total training rewards: 20.0 after n steps = 43 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 44 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 45 with final reward = 1.0\n",
            "Total training rewards: 21.0 after n steps = 46 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 26.0 after n steps = 47 with final reward = 1.0\n",
            "Total training rewards: 36.0 after n steps = 48 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 49 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 50 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 51 with final reward = 1.0\n",
            "Total training rewards: 34.0 after n steps = 52 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 14.0 after n steps = 53 with final reward = 1.0\n",
            "Total training rewards: 31.0 after n steps = 54 with final reward = 1.0\n",
            "Total training rewards: 15.0 after n steps = 55 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 56 with final reward = 1.0\n",
            "Total training rewards: 22.0 after n steps = 57 with final reward = 1.0\n",
            "Total training rewards: 24.0 after n steps = 58 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 11.0 after n steps = 59 with final reward = 1.0\n",
            "Total training rewards: 30.0 after n steps = 60 with final reward = 1.0\n",
            "Total training rewards: 52.0 after n steps = 61 with final reward = 1.0\n",
            "Total training rewards: 29.0 after n steps = 62 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 34.0 after n steps = 63 with final reward = 1.0\n",
            "Total training rewards: 30.0 after n steps = 64 with final reward = 1.0\n",
            "Total training rewards: 80.0 after n steps = 65 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 22.0 after n steps = 66 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 67 with final reward = 1.0\n",
            "Total training rewards: 38.0 after n steps = 68 with final reward = 1.0\n",
            "Total training rewards: 30.0 after n steps = 69 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 70 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 14.0 after n steps = 71 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 72 with final reward = 1.0\n",
            "Total training rewards: 17.0 after n steps = 73 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 74 with final reward = 1.0\n",
            "Total training rewards: 16.0 after n steps = 75 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 76 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 77 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 78 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 17.0 after n steps = 79 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 80 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 81 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 82 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 83 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 84 with final reward = 1.0\n",
            "Total training rewards: 18.0 after n steps = 85 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 86 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 19.0 after n steps = 87 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 88 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 89 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 90 with final reward = 1.0\n",
            "Total training rewards: 15.0 after n steps = 91 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 92 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 93 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 94 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 11.0 after n steps = 95 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 96 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 97 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 98 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 99 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 100 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 101 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 102 with final reward = 1.0\n",
            "Total training rewards: 8.0 after n steps = 103 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 9.0 after n steps = 104 with final reward = 1.0\n",
            "Total training rewards: 17.0 after n steps = 105 with final reward = 1.0\n",
            "Total training rewards: 16.0 after n steps = 106 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 107 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 108 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 109 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 110 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 111 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 112 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 10.0 after n steps = 113 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 114 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 115 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 116 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 117 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 118 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 119 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 120 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 121 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 122 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 12.0 after n steps = 123 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 124 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 125 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 126 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 127 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 128 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 129 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 130 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 131 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 132 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 10.0 after n steps = 133 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 134 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 135 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 136 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 137 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 138 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 139 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 140 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 141 with final reward = 1.0\n",
            "Total training rewards: 66.0 after n steps = 142 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 11.0 after n steps = 143 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 144 with final reward = 1.0\n",
            "Total training rewards: 8.0 after n steps = 145 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 146 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 147 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 148 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 149 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 150 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 151 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 152 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 11.0 after n steps = 153 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 154 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 155 with final reward = 1.0\n",
            "Total training rewards: 16.0 after n steps = 156 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 157 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 158 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 159 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 160 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 161 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 9.0 after n steps = 162 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 163 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 164 with final reward = 1.0\n",
            "Total training rewards: 9.0 after n steps = 165 with final reward = 1.0\n",
            "Total training rewards: 53.0 after n steps = 166 with final reward = 1.0\n",
            "Total training rewards: 74.0 after n steps = 167 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 39.0 after n steps = 168 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 169 with final reward = 1.0\n",
            "Total training rewards: 20.0 after n steps = 170 with final reward = 1.0\n",
            "Total training rewards: 65.0 after n steps = 171 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 65.0 after n steps = 172 with final reward = 1.0\n",
            "Total training rewards: 69.0 after n steps = 173 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 40.0 after n steps = 174 with final reward = 1.0\n",
            "Total training rewards: 13.0 after n steps = 175 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 176 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 177 with final reward = 1.0\n",
            "Total training rewards: 44.0 after n steps = 178 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 58.0 after n steps = 179 with final reward = 1.0\n",
            "Total training rewards: 12.0 after n steps = 180 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 181 with final reward = 1.0\n",
            "Total training rewards: 49.0 after n steps = 182 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 103.0 after n steps = 183 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 61.0 after n steps = 184 with final reward = 1.0\n",
            "Total training rewards: 78.0 after n steps = 185 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 57.0 after n steps = 186 with final reward = 1.0\n",
            "Total training rewards: 88.0 after n steps = 187 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 14.0 after n steps = 188 with final reward = 1.0\n",
            "Total training rewards: 72.0 after n steps = 189 with final reward = 1.0\n",
            "Total training rewards: 77.0 after n steps = 190 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 80.0 after n steps = 191 with final reward = 1.0\n",
            "Total training rewards: 108.0 after n steps = 192 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 9.0 after n steps = 193 with final reward = 1.0\n",
            "Total training rewards: 10.0 after n steps = 194 with final reward = 1.0\n",
            "Total training rewards: 11.0 after n steps = 195 with final reward = 1.0\n",
            "Total training rewards: 112.0 after n steps = 196 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 70.0 after n steps = 197 with final reward = 1.0\n",
            "Total training rewards: 85.0 after n steps = 198 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 45.0 after n steps = 199 with final reward = 1.0\n",
            "Total training rewards: 53.0 after n steps = 200 with final reward = 1.0\n",
            "Total training rewards: 88.0 after n steps = 201 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 16.0 after n steps = 202 with final reward = 1.0\n",
            "Total training rewards: 98.0 after n steps = 203 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 55.0 after n steps = 204 with final reward = 1.0\n",
            "Total training rewards: 106.0 after n steps = 205 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 74.0 after n steps = 206 with final reward = 1.0\n",
            "Total training rewards: 53.0 after n steps = 207 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 98.0 after n steps = 208 with final reward = 1.0\n",
            "Total training rewards: 61.0 after n steps = 209 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 91.0 after n steps = 210 with final reward = 1.0\n",
            "Total training rewards: 57.0 after n steps = 211 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 100.0 after n steps = 212 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 53.0 after n steps = 213 with final reward = 1.0\n",
            "Total training rewards: 55.0 after n steps = 214 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 88.0 after n steps = 215 with final reward = 1.0\n",
            "Total training rewards: 119.0 after n steps = 216 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 60.0 after n steps = 217 with final reward = 1.0\n",
            "Total training rewards: 113.0 after n steps = 218 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 132.0 after n steps = 219 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 15.0 after n steps = 220 with final reward = 1.0\n",
            "Total training rewards: 95.0 after n steps = 221 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 147.0 after n steps = 222 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 100.0 after n steps = 223 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 68.0 after n steps = 224 with final reward = 1.0\n",
            "Total training rewards: 97.0 after n steps = 225 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 90.0 after n steps = 226 with final reward = 1.0\n",
            "Total training rewards: 85.0 after n steps = 227 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 102.0 after n steps = 228 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 59.0 after n steps = 229 with final reward = 1.0\n",
            "Total training rewards: 85.0 after n steps = 230 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 57.0 after n steps = 231 with final reward = 1.0\n",
            "Total training rewards: 47.0 after n steps = 232 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 47.0 after n steps = 233 with final reward = 1.0\n",
            "Total training rewards: 36.0 after n steps = 234 with final reward = 1.0\n",
            "Total training rewards: 67.0 after n steps = 235 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 43.0 after n steps = 236 with final reward = 1.0\n",
            "Total training rewards: 137.0 after n steps = 237 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 103.0 after n steps = 238 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 135.0 after n steps = 239 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 60.0 after n steps = 240 with final reward = 1.0\n",
            "Total training rewards: 120.0 after n steps = 241 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 140.0 after n steps = 242 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 50.0 after n steps = 243 with final reward = 1.0\n",
            "Total training rewards: 114.0 after n steps = 244 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 43.0 after n steps = 245 with final reward = 1.0\n",
            "Total training rewards: 78.0 after n steps = 246 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 61.0 after n steps = 247 with final reward = 1.0\n",
            "Total training rewards: 77.0 after n steps = 248 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 135.0 after n steps = 249 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 15.0 after n steps = 250 with final reward = 1.0\n",
            "Total training rewards: 58.0 after n steps = 251 with final reward = 1.0\n",
            "Total training rewards: 87.0 after n steps = 252 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 41.0 after n steps = 253 with final reward = 1.0\n",
            "Total training rewards: 95.0 after n steps = 254 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 96.0 after n steps = 255 with final reward = 1.0\n",
            "Total training rewards: 81.0 after n steps = 256 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 46.0 after n steps = 257 with final reward = 1.0\n",
            "Total training rewards: 42.0 after n steps = 258 with final reward = 1.0\n",
            "Total training rewards: 131.0 after n steps = 259 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 81.0 after n steps = 260 with final reward = 1.0\n",
            "Total training rewards: 93.0 after n steps = 261 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 55.0 after n steps = 262 with final reward = 1.0\n",
            "Total training rewards: 154.0 after n steps = 263 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 97.0 after n steps = 264 with final reward = 1.0\n",
            "Total training rewards: 97.0 after n steps = 265 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 78.0 after n steps = 266 with final reward = 1.0\n",
            "Total training rewards: 91.0 after n steps = 267 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 103.0 after n steps = 268 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 116.0 after n steps = 269 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 142.0 after n steps = 270 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 107.0 after n steps = 271 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 80.0 after n steps = 272 with final reward = 1.0\n",
            "Total training rewards: 114.0 after n steps = 273 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 38.0 after n steps = 274 with final reward = 1.0\n",
            "Total training rewards: 115.0 after n steps = 275 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 130.0 after n steps = 276 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 96.0 after n steps = 277 with final reward = 1.0\n",
            "Total training rewards: 127.0 after n steps = 278 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 147.0 after n steps = 279 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 40.0 after n steps = 280 with final reward = 1.0\n",
            "Total training rewards: 161.0 after n steps = 281 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 106.0 after n steps = 282 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 129.0 after n steps = 283 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 123.0 after n steps = 284 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 126.0 after n steps = 285 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 109.0 after n steps = 286 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 154.0 after n steps = 287 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 52.0 after n steps = 288 with final reward = 1.0\n",
            "Total training rewards: 120.0 after n steps = 289 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 103.0 after n steps = 290 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 124.0 after n steps = 291 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 104.0 after n steps = 292 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 149.0 after n steps = 293 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 167.0 after n steps = 294 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 121.0 after n steps = 295 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 157.0 after n steps = 296 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 127.0 after n steps = 297 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 133.0 after n steps = 298 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 170.0 after n steps = 299 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSq8i4Q2XHJ-"
      },
      "source": [
        "%%bash\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lbznxsHcCYh"
      },
      "source": [
        "!apt-get install python-opengl -y\n",
        "\n",
        "!apt install xvfb -y\n",
        "\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install piglet\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "img = plt.imshow(env.render('rgb_array')) # only call this once\n",
        "for _ in range(40):\n",
        "    img.set_data(env.render('rgb_array')) # just update the data\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = env.action_space.sample()\n",
        "    env.step(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAcm0YqueMrB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}